# -*- coding: utf-8 -*-
"""
Title: 3D Calibration for Super-res
Concept: We assume that all files in a given folder are meant to help with the 
calibration process, this allows us to build a batch of localizations
Data: Raw Camera frames of latex-beads where each frame is displaced axially
by a known amount. Takes entire folder as a single dataset
Created on Mon Jun  8 09:16:14 2020


@author: Andrew Nelson
"""
#%% Import Libraries
from ryan_image_io import *
from localizations_class import *
from localization_kernels import *
from rolling_ball_subtraction import *
from scipy.ndimage import gaussian_filter1d
from scipy.interpolate import interp1d
import pickle
from data_visualization import *
import matplotlib.pyplot as plt
def collect_psfs_from_folders(folder, pixel_width = 5):
    '''This section takes in an image file, performs the regular pre-localization
    analysis, makes a sum projection, then identifies beads from surrounding area
    Once a bead is identified, it's image is segmented throughout all frames.'''
    split = 187
    # Generate list of image files
    image_files = grab_image_files(folder)
    # Loop over every image file
    #psfs = cut_out_psfs_from_file(image_files, loc1.pixel_width)
    # Background subtract, maximum project, PSF ID, then PSF segmentation
    # Store segmented PSFs
    psfs = []
    offset_index = []
    color = np.array([])
    for file in image_files:
        image_stack = load_image_to_array(folder + file)
        # Image is loaded
        background_subtracted_image_stack, bkgns =  rolling_ball_subtraction(image_stack,
                                                                         gauss_sigma = 2.5, 
                                                                         rolling_ball_radius = 6,
                                                                         rolling_ball_height= 6)
        m,n,o = image_size(image_stack)
        # Sum overall frames for psf detection
        image_stack[:,:,0] = background_subtracted_image_stack.sum(axis=2)
        image_stack[:10,:,0] = 0
        image_stack[-10:,:,0] = 0
        image_stack[:,:10,0] = 0
        image_stack[:,-10:,0] = 0
        waves = wavelet_denoising(image_stack[:,:,0:1])
        # Get a boolean map of peaks above threshold
        image2 = find_peaks(waves, threshold = 1000)
        # Get all peaks
        centers = count_peaks(image2)
        # only centers on the first page are interesting to us
        psf_peaks = centers[centers[:,2]==0,:]
        
        # Cut out all PSF images from background_subtracted_stacks
        color = np.append(color,psf_peaks[:,1] >= split) # Color Identifcation system 
        print(file)
        for peak in psf_peaks:
            psf_array = np.empty((2*pixel_width+1,2*pixel_width+1,o))
            index_array = np.empty((o,2))
            for frame in range(o):
                roi = background_subtracted_image_stack[peak[0]-pixel_width:peak[0]+pixel_width+1, # row
                                                        peak[1]-pixel_width:peak[1]+pixel_width+1, # col
                                                        frame] # frame
                # Refocus on the brightest pixel in the frame and record the shift
                index_array[frame,:] = np.argwhere(roi == roi.max())[0]
                # Our Identification algorithm will grab the bright pixel, so we should try to match
                psf_array[:,:,frame] = background_subtracted_image_stack[int(peak[0] - pixel_width + index_array[frame,0] - pixel_width):int(peak[0] - pixel_width + index_array[frame,0] + pixel_width + 1), # row
                                                                         int(peak[1] - pixel_width + index_array[frame,1] - pixel_width):int(peak[1] - pixel_width + index_array[frame,1] + pixel_width + 1), # col
                                                                         frame] # frame
                
            # Append this list of bright, centered PSFs    
            psfs.append(psf_array)
            offset_index.append(index_array)
    return psfs, offset_index, color

def get_eliptical_angle(image, pixel_width = 5):
    '''This function will make a mock eliptical guassian and subtract it from the
    provide image, A cost is generated by the sum of squared differences'''
    off_guess = image.min() # Estimate background
    peak_guess = image.max() # Estimate peak
    sigma_x = 1.25
    sigma_y = 2.54
    x = np.linspace(-pixel_width,pixel_width,2*pixel_width+1)
    X,Y = np.meshgrid(x,x)
    xcm = (X*image).sum()/image.sum()
    ycm = (Y*image).sum()/image.sum()
    C = np.empty(180)
    out_images = np.empty((image.shape[0],image.shape[1],180))
    for i in range(45, 45 + 180, 1):
        rx = (X-xcm)*np.cos(np.deg2rad(i)) - (Y-ycm)*np.sin(np.deg2rad(i))
        ry = (X-xcm)*np.sin(np.deg2rad(i)) + (Y-ycm)*np.cos(np.deg2rad(i))
        mock_image = peak_guess*np.exp(-rx**2/(2*sigma_x**2) - ry**2/(2*sigma_y**2)) + off_guess
        #out_images[:,:,i-] = (mock_image-image)**2
        
        C[i-45] = ((mock_image-image)**2).sum()
    #save_array_as_image(out_images, 'Rotation_example.tif') # This line saves the output of the elipticity measurement for user viewing
    min_angle = np.argwhere(C==C.min()) + 45 # Returning the minimum angle corresponds to lowest Cost
    return min_angle

def determine_elipticity_for_channels(psfs, color):
    '''Takes in a list of psfs and colors and outputs the angle we rotate our 
    coordinate grid for fitting'''
    m,n,o = psfs[0].shape
    orange_mols = np.argwhere(color == 1)
    red_mols = np.argwhere(color == 0)
    
    lower_quartile = round(o/4)

    orange_index = lower_quartile
    red_index = lower_quartile
   
    # Determine optimal angle of elipticity
    ang = np.empty(2)
    
    for i in range(2):# Looping angle measurement over 2 colors
        mols =  np.abs(i-color)
        ang[i] = 0
    image = psfs[orange_mols[0][0]][:,:,orange_index]
    orange_angs = np.empty(7)
    red_angs = np.empty(7)
    for frame in range(-3,4,1):
        orange_angs[frame + 3] = get_eliptical_angle(psfs[orange_mols[0][0]][:,:,orange_index+frame])
        red_angs[frame + 3] = get_eliptical_angle(psfs[red_mols[0][0]][:,:,red_index+frame])
    # Taking a straight mean can be problematic
    orange_angle = orange_angs[orange_angs>0].mean()
    red_angle = red_angs[red_angs>0].mean()
    if orange_angle > 180:
        orange_angle -= 180
    if red_angle > 180:
        red_angle -= 180
    return orange_angle, red_angle

def get_fitting_data(psfs, orange_angle, red_angle, color, offset_index):
    """Get Fitting data and store it in a list of localization classes"""
    locs = []
    max_succesful_fits = 0
    psf_index = -1
    for i in range(len(psfs)):
        psf = psfs[i]
        angle = np.ones(psf.shape[2])
        locs.append(Localizations('PSF {}'.format(i)))
        if color[i] == 1:
            angle *= orange_angle
        else:
            angle *= red_angle
        d_psfs = cp.asarray(psf)
        d_fitting_vectors = cp.empty((psf.shape[2],6))
        d_rotation = cp.asarray(angle)
    
        # Perform Fit
        fitting_kernel((1024,),( psf.shape[2]//1024 + 1,),(d_psfs, d_fitting_vectors, d_rotation, psf.shape[2], 20))
        fitted_vectors = cp.asnumpy(d_fitting_vectors)
        
        list_of_good_fits = remove_bad_fits(fitted_vectors)
        for j in range(2): # make offset correction
            fitted_vectors[:,j] += offset_index[i][:,1-j]
        
        frames = np.array(range(psf.shape[2]))
        frames = frames[list_of_good_fits]
        keep_vectors = fitted_vectors[list_of_good_fits,:]
        keep_psfs = psf[:,:,list_of_good_fits]
        # At this point fitting_vectors should contain coordinates of all fitted localizations in the dataset
        
        crlb_vectors = get_error_values(keep_psfs, keep_vectors) # perform a single run through of MLE to compute error values
    
        del d_psfs, d_fitting_vectors, d_rotation
        locs[i].store_fits(keep_vectors,crlb_vectors, frames)
        if keep_vectors.shape[0] > max_succesful_fits:
            psf_index = i
            max_succesful_fits = keep_vectors.shape[0]
        
    # Ok so now the locs variable is a list of localization classes which correspond to the 
    # At this point, we can call any list of localizations and fits by locs[i].xf
    for loc in locs: # Recenter all localizations by average position
        loc.xf -= loc.xf.mean()
        loc.yf -= loc.yf.mean()
        loc.zf = loc.frames*step_size_in_nanometer/1000 # store Z position as defined by experiment
    return locs

def correlate_axial_curves(locs, color, step_size_in_nanometer = 20):
    successful_fits = []
    # Show some Z stuff
    for i in range(len(psfs)):
        successful_fits.append(locs[i].sx.shape[0])
    
    successful_fits = np.array(successful_fits)
   

    # Synchronize Scans
    # First we have to choose a reference scan to correlate the others to
    # Given that we have multi-color data, this will have to be taken into account
    # Take reference scan for each color determined by number of successful fits
    # We can then compare each PSF to it's reference scan to get the correlation factor
    
    # Please watch as I do some fancy index swapping
    red_molecules = np.argwhere(color == 0)
    red_index = np.where(successful_fits[red_molecules] == 100)[0][0]
    orange_molecules = np.argwhere(color == 1)
    orange_index = np.where(successful_fits[orange_molecules] == 100)[0][0]
    
    # These next lines will be the models from which we will correlate everyone else
    red_model = locs[red_molecules[red_index][0]]
    orange_model = locs[orange_molecules[orange_index][0]]
  
    for i in range(len(locs)):
        loc = locs[i]
        # Build the reference sigma equations
        if color[i]:
            
            reference_model = orange_model
            reference_sigma_equation = gaussian_filter1d(orange_model.sx**2 - orange_model.sy**2, 4)
        else:
            
            reference_model = red_model
            reference_sigma_equation = gaussian_filter1d(red_model.sx**2 - red_model.sy**2, 4)
        current_sigma_equation = gaussian_filter1d(loc.sx**2 - loc.sy**2, 4)
        
        
        # We will grab the central 11 data points of the sigma equations
        # Then plotting the eq. against frame / z position, we can line up the 
        # molecules
        
        m = int(np.round(reference_sigma_equation.shape[0]/2))
        
        reference_sigma_segment = reference_sigma_equation[m-5:m+6]
        current_sigma_segment = current_sigma_equation[m-5:m+6]
        reference_time = reference_model.frames[m-5:m+6]
        current_time = loc.frames[m-5:m+6]
        
        
        # Perform a linear fit to the data
        try:
            reference_model = np.polyfit(reference_sigma_segment, reference_time, 1)
            current_model = np.polyfit(current_sigma_segment, current_time,  1)
            
            loc.frame_offset = round((reference_model[1] - current_model[1]))
        except:
            loc.frame_offset = 0
        loc.zf = ((loc.frames + loc.frame_offset)*step_size_in_nanometer/1000)

def build_axial_sigma_models(locs, results_to_save, color):
    red_molecules = np.argwhere(color == 0)
    orange_molecules = np.argwhere(color == 1)
    # build a single array of sigma values
    all_red_zs = np.array([])
    all_red_x_sigmas = np.array([])
    all_red_y_sigmas = np.array([])
    all_orange_zs = np.array([])
    all_orange_x_sigmas = np.array([])
    all_orange_y_sigmas = np.array([])
    for i in range(len(locs)):
        loc = locs[i]
        if color[i]:
            all_orange_zs = np.append(all_orange_zs, loc.zf)
            all_orange_x_sigmas = np.append(all_orange_x_sigmas, loc.sx)
            all_orange_y_sigmas = np.append(all_orange_y_sigmas, loc.sy)
        else:
            all_red_zs = np.append(all_red_zs, loc.zf)
            all_red_x_sigmas = np.append(all_red_x_sigmas, loc.sx)
            all_red_y_sigmas = np.append(all_red_y_sigmas, loc.sy)
    # Hey at this point we can treat all of these together because they're independent now
    
    list_of_unique_orange_zs = np.unique(all_orange_zs)
    list_of_unique_red_zs = np.unique(all_red_zs)
    
    model_orange_sx = np.array([])
    model_orange_sy = np.array([])
    model_red_sx = np.array([])
    model_red_sy = np.array([])
    for z in list_of_unique_orange_zs:
        sub_zs = np.argwhere(all_orange_zs == z) # Find all indices that correspond to the current height
        
        sub_sigma_x = all_orange_x_sigmas[sub_zs]
        sub_sigma_y = all_orange_y_sigmas[sub_zs]
    
        model_orange_sx = np.append(model_orange_sx,sub_sigma_x.mean())
        model_orange_sy = np.append(model_orange_sy,sub_sigma_y.mean())
    
    for z in list_of_unique_red_zs:
        sub_zs = np.argwhere(all_red_zs == z) # Find all indices that correspond to the current height
        
        sub_sigma_x = all_red_x_sigmas[sub_zs]
        sub_sigma_y = all_red_y_sigmas[sub_zs]
    
        model_red_sx = np.append(model_red_sx,sub_sigma_x.mean())
        model_red_sy = np.append(model_red_sy,sub_sigma_y.mean())
    initial_point = 0.75 # This is a very low side estimate of where the curve interesction point is
    
    orange_differential_sigma = np.abs(model_orange_sx - model_orange_sy)
    orange_initial_index = np.argwhere(np.abs(list_of_unique_orange_zs - initial_point) <= 0.02)[0][0]
    orange_cofocal_index = np.argwhere(orange_differential_sigma[orange_initial_index:orange_initial_index + 20] == orange_differential_sigma[orange_initial_index:orange_initial_index + 20].min())[0][0] + orange_initial_index

    list_of_unique_orange_zs -= list_of_unique_orange_zs[orange_cofocal_index]
    
    red_differential_sigma = np.abs(model_red_sx - model_red_sy)
    red_initial_index = np.argwhere(np.abs(list_of_unique_red_zs - initial_point) <= 0.02)[0][0]
    red_cofocal_index = np.argwhere(red_differential_sigma[red_initial_index: red_initial_index + 20] == red_differential_sigma[red_initial_index:red_initial_index + 20].min())[0][0] + red_initial_index

    list_of_unique_red_zs -= list_of_unique_red_zs[red_cofocal_index]
    # Smooth Curves
    model_orange_sx = gaussian_filter1d(model_orange_sx,3)
    model_red_sx = gaussian_filter1d(model_red_sx,3)
    model_orange_sy = gaussian_filter1d(model_orange_sy,3)
    model_red_sy = gaussian_filter1d(model_red_sy,3)
    
    
    # Store the spline interpolation of the curves for use later
    results_to_save['model_orange_sx'] = interp1d(list_of_unique_orange_zs, model_orange_sx, kind = 'cubic')
    results_to_save['model_red_sx'] =  interp1d(list_of_unique_red_zs, model_red_sx, kind = 'cubic')
    results_to_save['model_orange_sy'] = interp1d(list_of_unique_orange_zs, model_orange_sy, kind = 'cubic')
    results_to_save['model_red_sy'] =  interp1d(list_of_unique_red_zs, model_red_sy, kind = 'cubic')


    
#%% Main Workspace
if __name__ == '__main__':
    folder = 'G:\\Dropbox\\Data\\1-20-21 calibration\\'
    #fpath = 'D:\\Dropbox\\Data\\6-23-20 calibrations\\Pre_prepped_images\\'
    # We are storing regularly used values in the localization class
    step_size_in_nanometer = 20
    psfs, offset_index, color = collect_psfs_from_folders(folder)
    # At this point, PSFS should be a list of numpy arrays that contain the PSFS that will subsequently be fitted
    # Meanwhile color should be a boolean list of the same size representing color of the psf, which determines the 
    # final fit calibration
    #%%
    orange_angle, red_angle = determine_elipticity_for_channels(psfs, color)
    
    # We can stor results in a dictionary
    results_to_save = {'orange_angle' : orange_angle,
                       'red_angle' : red_angle}
    
    # Fit those PSFs
    locs = get_fitting_data(psfs, orange_angle, red_angle, color, offset_index)
    
    correlate_axial_curves(locs, color, step_size_in_nanometer) 
    
    build_axial_sigma_models(locs, results_to_save, color)
    
    # Show me what you got!
    x = np.linspace(-0.8,0.8,1600)
    plt.plot(x,results_to_save['model_orange_sx'](x), label = 'Orange sig x')
    plt.plot(x,results_to_save['model_orange_sy'](x), label = 'Orange sig y')
    plt.plot(x,results_to_save['model_red_sx'](x), label = 'Red sig x')
    plt.plot(x,results_to_save['model_red_sy'](x), label = 'Red sig y')
    plt.legend()
    plt.title('PSF-width as a function of axial position')
    plt.xlabel('Relative axial location (um)')
    plt.ylabel('Width of parameter (um)')

    # So now we've got spline interpolations of our sigma functions, this will allow us to pinpoint our Z position
    orange_xs = np.array([])
    orange_ys = np.array([])
    red_xs = np.array([])
    red_ys = np.array([])
    red_zs = np.array([])
    orange_zs = np.array([])
    orange_frames = np.array([])
    red_frames = np.array([])
    for i in range(len(locs)):
        loc = locs[i]
        if color[i]:
            sigx_model = results_to_save['model_orange_sx'](x)
            sigy_model = results_to_save['model_orange_sy'](x)
        else:
            sigx_model = results_to_save['model_red_sx'](x)
            sigy_model = results_to_save['model_red_sy'](x)
        for j in range(loc.xf.shape[0]): # loop over all them fits
            sigma_distance = ((sigx_model**0.5 - loc.sx[j]**0.5)**2 + 
                              (sigy_model**0.5 - loc.sy[j]**0.5)**2)**0.5
            x_index = np.argwhere(sigma_distance == sigma_distance.min())[0][0]
            loc.zf[j] = x[x_index]
            
        mid_zs_index = np.argwhere(np.abs(loc.zf) < 0.3)
        loc.xf -= loc.xf[mid_zs_index].mean()
        loc.yf -= loc.yf[mid_zs_index].mean()
        if color[i]:
            orange_xs = np.append(orange_xs,loc.xf)
            orange_ys = np.append(orange_ys,loc.yf)
            orange_zs = np.append(orange_zs,loc.zf)
            orange_frames = np.append(orange_frames,loc.frames)
        else:
            red_xs = np.append(red_xs,loc.xf)
            red_ys = np.append(red_ys,loc.yf)
            red_zs = np.append(red_zs,loc.zf)
            red_frames = np.append(red_frames,loc.frames)
            
    # At this point, locs coords have been updated such the the 'cofocal' region of these curves are centered at (0,0,0) um
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d') 

    ax.scatter(red_xs, red_ys, red_zs, 'r', label = 'Red')
    ax.scatter(orange_xs, orange_ys, orange_zs, 'b', label = 'Orange')
    ax.legend()
    #%%
    orange_search_zs = np.linspace(orange_zs.min(),orange_zs.max(),20)
    orange_spline_xs = np.empty(orange_search_zs.shape[0] -1)
    orange_spline_ys = np.empty(orange_search_zs.shape[0] -1)
    orange_spline_zs = np.empty(orange_search_zs.shape[0] -1)
    
    for i in range(orange_search_zs.shape[0]-1):
        index_within_z = np.argwhere(np.abs(orange_zs - (orange_search_zs[i] + orange_search_zs[i+1])/2) <= orange_search_zs[i+1] - orange_search_zs[i])
        X_set = orange_xs[index_within_z]
        Y_set = orange_ys[index_within_z]
        Z_set = orange_zs[index_within_z]
        
        r = ((X_set-X_set.mean())**2 + (Y_set- Y_set.mean())**2)**0.5/locs[0].pixel_size
        within_range = np.argwhere(r < 0.5)[:,0]
        orange_spline_xs[i] = X_set[within_range].mean()
        orange_spline_ys[i] = Y_set[within_range].mean()
        orange_spline_zs[i] = Z_set[within_range].mean()

    
    red_search_zs = np.linspace(red_zs.min(),red_zs.max(),20)
    red_spline_xs = np.empty(red_search_zs.shape[0] -1)
    red_spline_ys = np.empty(red_search_zs.shape[0] -1)
    red_spline_zs = np.empty(red_search_zs.shape[0] -1)
    
    for i in range(red_search_zs.shape[0]-1):
        index_within_z = np.argwhere(np.abs(red_zs - (red_search_zs[i] + red_search_zs[i+1])/2) <= red_search_zs[i+1] - red_search_zs[i])
        X_set = red_xs[index_within_z]
        Y_set = red_ys[index_within_z]
        Z_set = red_zs[index_within_z]
        
        r = ((X_set-X_set.mean())**2 + (Y_set- Y_set.mean())**2)**0.5/locs[0].pixel_size
        within_range = np.argwhere(r < 0.5)[:,0]
        red_spline_xs[i] = X_set[within_range].mean()
        red_spline_ys[i] = Y_set[within_range].mean()
        red_spline_zs[i] = Z_set[within_range].mean()
        


        
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d') 

    ax.plot(orange_spline_xs, orange_spline_ys, orange_spline_zs, 'b', label = 'Average Measurement')
    ax.scatter(orange_xs, orange_ys, orange_zs, 'c', label = 'Orange_scatter')
    ax.legend()
    
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d') 

    ax.plot(gaussian_filter1d(red_spline_xs, 1), gaussian_filter1d(red_spline_ys, 1), red_spline_zs, 'b', label = 'Average Measurement')
    ax.scatter(red_xs, red_ys, red_zs, 'r', label = 'Red Scatter')
    ax.legend()
    
    # Gaussian Smoothing over the intervals for X Y spline
    red_spline_xs = gaussian_filter1d(red_spline_xs, 1)
    red_spline_ys = gaussian_filter1d(red_spline_ys, 1)
    
    # Gaussian Smoothing over the intervals for X Y spline
    orange_spline_xs = gaussian_filter1d(orange_spline_xs, 1)
    orange_spline_ys = gaussian_filter1d(orange_spline_ys, 1)
    
    # Save results as interpolations to the results dictionary
    results_to_save['model_orange_x_axial_correction'] = interp1d(orange_spline_zs, 
                                                                  orange_spline_xs, 
                                                                  kind = 'cubic',
                                                                  bounds_error = False)
    results_to_save['model_orange_y_axial_correction'] = interp1d(orange_spline_zs, 
                                                                  orange_spline_ys, 
                                                                  kind = 'cubic',
                                                                  bounds_error = False)
    results_to_save['model_red_x_axial_correction'] = interp1d(red_spline_zs, 
                                                               red_spline_xs, 
                                                               kind = 'cubic',
                                                               bounds_error = False)
    results_to_save['model_red_y_axial_correction'] = interp1d(red_spline_zs, 
                                                               red_spline_ys, 
                                                               kind = 'cubic',
                                                               bounds_error = False)
    
    
    sub_reds = np.argwhere(np.abs(red_zs) <= 0.8)
    red_x_corrections = results_to_save['model_red_x_axial_correction'](red_zs[sub_reds])
    red_y_corrections = results_to_save['model_red_y_axial_correction'](red_zs[sub_reds])
    
    sub_oranges = np.argwhere(np.abs(orange_zs) <= 1)
    orange_x_corrections = results_to_save['model_orange_x_axial_correction'](orange_zs[sub_oranges])
    orange_y_corrections = results_to_save['model_orange_y_axial_correction'](orange_zs[sub_oranges])
    
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d') 

    ax.scatter(orange_xs[sub_oranges] - orange_x_corrections, orange_ys[sub_oranges] - orange_y_corrections, orange_zs[sub_oranges], 'c', label = 'With Correction')
    ax.legend()

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d') 

    ax.scatter(red_xs[sub_reds] - red_x_corrections, red_ys[sub_reds] - red_y_corrections, red_zs[sub_reds], 'c', label = 'With Correction')
    ax.legend()

    # Take into account the index of refraction mismatch by plotting measured Z versus known Z
    orange_index = np.argwhere(np.abs(orange_frames*0.02 - 1) <= 0.25)
    orange_refraction_correction = np.polyfit(orange_frames[orange_index[:,0]]*0.02,
                                              orange_zs[orange_index[:,0]], 
                                              1)[0]
    red_index = np.argwhere(np.abs(red_frames*0.02 - 1) <= 0.25)
    red_refraction_correction = np.polyfit(red_frames[red_index[:,0]]*0.02,
                                              red_zs[red_index[:,0]], 
                                              1)[0]
    orange_zs /= orange_refraction_correction
    red_zs /= red_refraction_correction
    results_to_save['orange_refraction_correction'] = orange_refraction_correction
    results_to_save['red_refraction_correction'] = red_refraction_correction
    with open('C:\\Users\\andre\\Documents\\GitHub\\Python-Master\\Hurricane\\3d_calibration.pkl', 'wb') as f:
        pickle.dump(results_to_save,f)

